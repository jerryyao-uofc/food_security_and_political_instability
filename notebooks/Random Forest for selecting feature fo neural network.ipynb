{"cells":[{"cell_type":"code","execution_count":null,"id":"0a1b7081","metadata":{"id":"0a1b7081"},"outputs":[],"source":["# basic package\n","import csv\n","import glob\n","import pandas as pd\n","import matplotlib as plt\n","\n","import numpy as np\n","import random\n","from operator import itemgetter\n","import matplotlib.pyplot as plt\n","\n","\n","\n","\n","# ml related \n","from sklearn.metrics import r2_score\n","from sklearn import linear_model\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.inspection import permutation_importance\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error"]},{"cell_type":"code","execution_count":null,"id":"a7c9c43c","metadata":{"id":"a7c9c43c","outputId":"a8a591e9-7f8f-4c79-eaf3-655366a73ce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Functionality.ipynb                 political_selected.csv\r\n","Troll full data.pdf                 sued_full_dataset.pdf\r\n","\u001b[34mdrive-download-20220521T173459Z-001\u001b[m\u001b[m undernourish_selected.csv\r\n"]}],"source":["\n"]},{"cell_type":"code","execution_count":null,"id":"a99c8ab4","metadata":{"id":"a99c8ab4"},"outputs":[],"source":["political = pd.read_csv(\"political_selected.csv\")\n","undernourish = pd.read_csv(\"undernourish_selected.csv\")\n"]},{"cell_type":"markdown","id":"40f72377","metadata":{"id":"40f72377"},"source":["# Function Definitions for Random forest\n"]},{"cell_type":"code","execution_count":null,"id":"3ced5b79","metadata":{"id":"3ced5b79"},"outputs":[],"source":["# functions you will need \n","\n","def map_code_to_meaning(mapping, code_no):\n","    return (mapping.loc[mapping['code']==code_no]).iloc[0].var_name\n","\n","# print the total percetnage of missing in each dataset\n","def total_percentage_missing(df):\n","    return(np.count_nonzero(df.isna()) / df.size)\n","\n","# drop the top N rows with most NAs\n","def drop_top_N_rows_with_most_NAs(df, N=300):\n","    if N/len(df)> 0.2:\n","      warnings.warn(\"Based on your speficied N, you are dropping more then 20% of the data\")\n","\n","    print(\"shape before drop\", df.shape)\n","    dict_nas = {}\n","    for i in range(len(df)):\n","        percentage = total_percentage_missing(df.iloc[i])\n","        dict_nas[i] = percentage\n","    res = dict(sorted(dict_nas.items(), key = itemgetter(1), reverse = True)[:N])\n","    # print(\"here\")\n","    top_NAs_rows = list(res.keys())\n","    # print(top_NAs_rows)\n","    df.drop(top_NAs_rows, axis=0, inplace=True)\n","    print(\"shape after drop \", df.shape)\n","    print(\"Missing data percentage \", total_percentage_missing(df) )\n","    return df\n","\n","# split into two dataset by year (default=2017)\n","# fist one include that year, second one is year after that\n","def split_by_year(df, split_at = 2017):\n","    res1 = df.loc[df['Year']<= split_at]\n","    res2 = df.loc[df['Year']>= split_at]\n","    return res1, res2\n","\n","def print_all_coeff(list_coef, feature_name):\n","    sort_index = reversed(np.argsort(list_coef))\n","    list_of_lists = []\n","    for i in sort_index:\n","#         print(feature_name[i])\n","        temp = int(feature_name[i])\n","        if list_coef[i] !=0.0:\n","            list_of_lists.append([round(list_coef[i],10), feature_name[i], map_code_to_meaning(meaning_map, temp)])\n","    return pd.DataFrame(list_of_lists, columns =['non_zero_coefficient', 'code', 'variable_name'])\n","\n","  # pipeline on returning the coefficient of lasso regression\n","# also returns the score of the regressions\n","def lasso_pipeline_for_cross_validation(df, target_name = 'political', split_year = 2017, lasso_alpha = 0.12):\n","\n","    if target_name not in df.columns:\n","        raise ValueError(\"The input dataframe doesn't have the column: political\")\n","    \n","    if 'Continent' in df.columns:\n","      df = df.drop(columns =['Continent'])\n","\n","    # default split at 2017\n","    political_pre_2017, political_post_2017 = split_by_year(df, split_at = split_year)\n","    \n","    # Note, the variable names here is only names, y_politcal can be any dataframe\n","    # doesn't have to be political \n","    y_political = political_pre_2017.pop(target_name)\n","    X_political = political_pre_2017.drop(columns = ['Year', 'Area Code'])\n","\n","    y_political_test = political_post_2017.pop(target_name)\n","    X_political_test = political_post_2017.drop(columns = ['Year', 'Area Code'])\n","\n","    \n","    feature_names = X_political_test.columns\n","\n","    # scale the X\n","    scaler = StandardScaler()\n","    political_scaler_X = scaler.fit(X_political)\n","    X_political_scaled = political_scaler_X.transform(X_political)\n","    X_political_test_scaled = political_scaler_X.transform(X_political_test)\n","\n","    # scale the y\n","    y_political = y_political.values.reshape(-1,1)\n","    y_political_test = y_political_test.values.reshape(-1,1)\n","    political_scaler_y = scaler.fit(y_political)\n","    y_political_scaled = political_scaler_y.transform(y_political)\n","    y_political_test_scaled = political_scaler_y.transform(y_political_test)\n","    \n","    # Run LASSO\n","    reg = linear_model.Lasso(alpha=lasso_alpha).fit(X_political_scaled, y_political_scaled)\n","    \n","    \n","    y_train_pred = reg.predict(X_political_scaled) # predicting for training\n","    y_pred = reg.predict(X_political_test_scaled)  # predicting for testing\n","    \n","    \n","    r2_training = r2_score(y_political_scaled,y_train_pred)\n","    r2_resting = r2_score(y_political_test_scaled, y_pred)\n","    \n","   \n","    MAE_train = mean_absolute_error(y_political_scaled,y_train_pred)\n","    MAE_test = mean_absolute_error(y_political_test_scaled, y_pred)\n","    \n","    return MAE_train, r2_training, MAE_test, r2_resting"]},{"cell_type":"code","execution_count":null,"id":"083aa2b3","metadata":{"scrolled":true,"id":"083aa2b3","outputId":"226947c5-216c-4dd8-f491-e59e2b2b8068"},"outputs":[{"name":"stdout","output_type":"stream","text":["shape before drop (3705, 1002)\n","shape after drop  (3405, 1002)\n","Missing data percentage  0.003907896395168547\n","shape before drop (3933, 1002)\n","shape after drop  (3633, 1002)\n","Missing data percentage  0.02336230374373741\n"]}],"source":["political = drop_top_N_rows_with_most_NAs(df= political, N= 300)\n","undernourish = drop_top_N_rows_with_most_NAs(df= undernourish, N= 300)\n","\n","\n","\n","\n","# fill NAs\n","political = political.fillna(0)\n","undernourish = undernourish.fillna(0)\n"]},{"cell_type":"code","execution_count":null,"id":"56a14ea4","metadata":{"id":"56a14ea4"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"850dfd36","metadata":{"id":"850dfd36"},"source":["# Random Forest"]},{"cell_type":"code","execution_count":null,"id":"7da26695","metadata":{"id":"7da26695"},"outputs":[],"source":["def findElements(lst1, lst2):\n","    return [lst1[i] for i in lst2]\n","\n","def reandom_forest(df, target_name = 'political', split_year = 2017):\n","\n","    if target_name not in df.columns:\n","        raise ValueError(\"The input dataframe doesn't have the column: political\")\n","    \n","    if 'Continent' in df.columns:\n","        df = df.drop(columns =['Continent'])\n","\n","    # default split at 2017\n","    political_pre_2017, political_post_2017 = split_by_year(df, split_at = split_year)\n","    \n","    # Note, the variable names here is only names, y_politcal can be any dataframe\n","    # doesn't have to be political \n","    y_political = political_pre_2017.pop(target_name)\n","    X_political = political_pre_2017.drop(columns = ['Year', 'Area Code'])\n","\n","    y_political_test = political_post_2017.pop(target_name)\n","    X_political_test = political_post_2017.drop(columns = ['Year', 'Area Code'])\n","\n","    \n","    feature_names = X_political_test.columns\n","\n","    # scale the X\n","    scaler = StandardScaler()\n","    political_scaler_X = scaler.fit(X_political)\n","    X_political_scaled = political_scaler_X.transform(X_political)\n","    X_political_test_scaled = political_scaler_X.transform(X_political_test)\n","\n","    # scale the y\n","    y_political = y_political.values.reshape(-1,1)\n","    y_political_test = y_political_test.values.reshape(-1,1)\n","    political_scaler_y = scaler.fit(y_political)\n","    y_political_scaled = political_scaler_y.transform(y_political)\n","    y_political_test_scaled = political_scaler_y.transform(y_political_test)\n","    \n","    # print shapes\n","    print(\"Training Shape:\", X_political_scaled.shape)\n","    print(\"Testing Shape\", X_political_test_scaled.shape)\n","    \n","    # Run LASSO\n","    random.seed(1234)\n","    rf = RandomForestRegressor(n_estimators=80)\n","    print(y_political_scaled.shape)\n","    rf.fit(X_political_scaled, y_political_scaled.ravel())\n","    \n","    std = np.std([rf.feature_importances_ for tree in rf.estimators_],\n","             axis=0)\n","    \n","    y_train_pred = rf.predict(X_political_scaled)\n","    y_test_pred = rf.predict(X_political_test_scaled)\n","    print(\"Mean Absolute Error on training\", mean_absolute_error(y_political_scaled, y_train_pred))\n","    print(\"Mean Absolute Error on testing\",  mean_absolute_error(y_political_test_scaled, y_test_pred))\n","    return rf.feature_importances_, feature_names, std\n","    \n","\n","#     # evaluation\n","#     print(\"score on training dataset\", reg.score(X_political_scaled, y_political_scaled) )\n","#     print(\"score on testing dataset\", reg.score(X_political_test_scaled, y_political_test_scaled))\n","#     y_train_pred = reg.predict(X_political_scaled)\n","#     y_pred = reg.predict(X_political_test_scaled)\n","#     print(\"R squared score on training\", r2_score(y_political_scaled, y_train_pred))\n","#     print(\"R squared score on testing\", r2_score(y_political_test, y_pred))\n","   \n","\n","#     res_df = print_all_coeff(reg.coef_, feature_names)\n","#     return res_df"]},{"cell_type":"code","execution_count":null,"id":"95d21ef7","metadata":{"scrolled":false,"id":"95d21ef7","outputId":"a4d93540-9a24-4afe-e7af-ca471770d6c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","Training Shape: (3041, 999)\n","Testing Shape (546, 999)\n","(3041, 1)\n","Mean Absolute Error on training 0.06150192643695166\n","Mean Absolute Error on testing 0.13487458937446906\n"]}],"source":["print(\"hi\")\n","res, names , stds = reandom_forest(political, target_name = 'political')"]},{"cell_type":"code","execution_count":null,"id":"f508a348","metadata":{"id":"f508a348"},"outputs":[],"source":["N = 50\n","import_f = sorted(range(len(res)), key = lambda sub: res[sub])[-N:]\n","\n","important_code = findElements(names,import_f )\n","important_stds = findElements(list(stds), import_f)\n","\n","meanings_import = []\n","\n","for each in important_code: \n","    \n","    meanings_import.append(each)\n"," "]},{"cell_type":"code","execution_count":null,"id":"58b63686","metadata":{"id":"58b63686"},"outputs":[],"source":["political_filterd = political.filter(items = meanings_import)"]},{"cell_type":"code","execution_count":null,"id":"4c5fd144","metadata":{"id":"4c5fd144","outputId":"8eedc715-e6fa-4a64-cc31-a3770f9c806c"},"outputs":[{"data":{"text/plain":["['69897230',\n"," '6990723112',\n"," '17385312',\n"," '68177230',\n"," '680072184',\n"," '69917273',\n"," '69907273',\n"," '69745008',\n"," '1664',\n"," '69785008',\n"," '6990723114',\n"," '6990723113',\n"," '69917225',\n"," '68217266',\n"," '68187225',\n"," '69725008',\n"," '65157225',\n"," '68217225',\n"," '65157273',\n"," '19545622',\n"," '17355312',\n"," '68187266',\n"," '6996726313',\n"," '66465110',\n"," '69815008',\n"," '6818723113',\n"," '6821726313',\n"," '69835007',\n"," '68007273',\n"," '66507208',\n"," '188294',\n"," '230826110',\n"," '17527213',\n"," '2041152',\n"," '17525118',\n"," '66505110',\n"," '3010512',\n"," '66217208',\n"," '68177266',\n"," '68187265',\n"," '65177225',\n"," '65077265',\n"," '69705008',\n"," '3010511',\n"," '2264',\n"," '3010551',\n"," '66705110',\n"," '202972380',\n"," '4635510',\n"," '220086185']"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["meanings_import"]},{"cell_type":"markdown","id":"9c2f6e4d","metadata":{"id":"9c2f6e4d"},"source":["# Undernourish"]},{"cell_type":"code","execution_count":null,"id":"e57e2206","metadata":{"id":"e57e2206","outputId":"d37025eb-582d-4200-c695-110fe5ecc315"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Shape: (3055, 999)\n","Testing Shape (771, 999)\n","(3055, 1)\n","Mean Absolute Error on training 0.04151812080248604\n","Mean Absolute Error on testing 0.1726539659023384\n"]}],"source":["res, names , stds = reandom_forest(undernourish, target_name = 'undernourish')"]},{"cell_type":"code","execution_count":null,"id":"91b11af7","metadata":{"id":"91b11af7"},"outputs":[],"source":["N = 50\n","import_f = sorted(range(len(res)), key = lambda sub: res[sub])[-N:]\n","\n","important_code = findElements(names,import_f )\n","important_stds = findElements(list(stds), import_f)\n","\n","meanings_import = []\n","\n","for each in important_code: \n","    \n","    meanings_import.append(each)\n"," "]},{"cell_type":"code","execution_count":null,"id":"4c678874","metadata":{"id":"4c678874","outputId":"30abc163-67d3-436e-f9a3-a1446782ec35"},"outputs":[{"data":{"text/plain":["['17385419',\n"," '622465',\n"," '67515110',\n"," '17735922',\n"," '67607246',\n"," '65057273',\n"," '188295',\n"," '1802465',\n"," '185610',\n"," '180294',\n"," '17735910',\n"," '17805420',\n"," '1890462',\n"," '173464',\n"," '5155610',\n"," '25764',\n"," '1664',\n"," '17205419',\n"," '65664',\n"," '184795',\n"," '622462',\n"," '67917245',\n"," '220306161',\n"," '4635312',\n"," '65157230',\n"," '184864',\n"," '67917225',\n"," '220306192',\n"," '68247273',\n"," '4065622',\n"," '67937245',\n"," '67917230',\n"," '104164',\n"," '16305510',\n"," '2203061930',\n"," '17805318',\n"," '67917246',\n"," '65177230',\n"," '6575622',\n"," '17175419',\n"," '18955910',\n"," '12325910',\n"," '6824723113',\n"," '65057225',\n"," '10165111',\n"," '65965',\n"," '184794',\n"," '8825420',\n"," '4635510',\n"," '17355419']"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["meanings_import"]},{"cell_type":"code","execution_count":null,"id":"0ef09d7b","metadata":{"id":"0ef09d7b"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Random Forest in preparation for neural network.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}